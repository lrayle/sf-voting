{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook runs the SF election data preparation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import openpyxl\n",
    "from xlrd import open_workbook\n",
    "from datetime import date\n",
    "\n",
    "from geopandas import GeoDataFrame, read_file\n",
    "\n",
    "import data_prep_functions as dpf\n",
    "from spatial_processing_functions import load_prec_shp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from election result .xls files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = '../data/SOV_w_nimby/'\n",
    "\n",
    "# read all files in the folder\n",
    "# keep data in a dictionary, where keys are election date (e.g., 200111)\n",
    "file_list = os.listdir(datapath)\n",
    "vote_data = {}\n",
    "for f in file_list:\n",
    "    d=f.strip('SOV').strip('.xls')\n",
    "    d=d[:-2]\n",
    "    if d[0]=='9':\n",
    "        d='19'+d\n",
    "    else:\n",
    "        d='20'+d\n",
    "    vote_data[d]={}\n",
    "    vote_data[d]['filename'] =f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define regex search term that will match name of worksheet with local propositions\n",
    "phrase = 'prop|meas'\n",
    "phrase_state = 'state|st'\n",
    "\n",
    "for d in vote_data.keys(): \n",
    "    f=vote_data[d]['filename']\n",
    "    # first only do .xls files.. because need different package to read these.\n",
    "    if f.split('.')[1]=='xls':      \n",
    "        #print(f)\n",
    "        wkbk=open_workbook(datapath+f)\n",
    "        \n",
    "        # Find the sheet(s) with local ballot proposals. \n",
    "        sheets_w_props=dpf.find_matching_sheets(wkbk, to_match=phrase, to_not_match=phrase_state)\n",
    "        vote_data[d]['sheet_names'] = sheets_w_props\n",
    "\n",
    "                \n",
    "# Here are ones with weird formats. \n",
    "vote_data['199711']['sheet_names']=['A - D','E - F']\n",
    "vote_data['199706']['sheet_names']=['970603']\n",
    "vote_data['199911']['sheet_names']=['E to H','I to K']\n",
    "\n",
    "vote_data['201411']['sheet_names']=['370 - Local Measure F']\n",
    "vote_data['201511']['sheet_names']=['180 - Local Measure D','205 - Local Measure I']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now find the data within each sheet\n",
    "\n",
    "# Define the ballot props we want for each election:\n",
    "proposals = pd.read_excel('../data/BallotPropositions_nimby2.xlsx')  \n",
    "proposals.head()\n",
    "proposals['Year_str']=proposals['Year'].astype(str)\n",
    "proposals['Mo_str']=proposals['Month2'].astype(str)\n",
    "\n",
    "for i,d in enumerate(proposals['Mo_str']):\n",
    "    if len(d)==1:\n",
    "        d='0'+d\n",
    "    proposals.loc[i,'Mo_str']=d\n",
    "\n",
    "proposals['Date_str']=proposals['Year_str']+proposals['Mo_str']\n",
    "#proposals.head()\n",
    "\n",
    "for d in vote_data.keys():\n",
    "    p_list=list(proposals[proposals['Date_str']==d]['Letter'].values)\n",
    "    vote_data[d]['props'] = dict.fromkeys(p_list,{})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Edit sheetnames so only the ones we need are listed\n",
    "vote_data['199711']['sheet_names']=['E - F']\n",
    "vote_data['199811']['sheet_names']= ['City Prop A-E']\n",
    "vote_data['200011']['sheet_names']=['Prop K-O']\n",
    "vote_data['200111']['sheet_names']=['AMENDMENTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has multiple sheets: 201311\n",
      "has multiple sheets: 201511\n",
      "has multiple sheets: 199911\n"
     ]
    }
   ],
   "source": [
    "# find the data for that prop in each sheet\n",
    "for d in vote_data.keys():\n",
    "    #print(d)\n",
    "    letters=list(vote_data[d]['props'].keys())\n",
    "    # we want one sheet for each prop letter\n",
    "\n",
    "    # I think I should handle them differently if there are multiple props\n",
    "    # there might be multiple letters, there might be multiple sheets, or both. \n",
    "    # but if there's only one letter, there's only one sheet.\n",
    "    if len(letters) >1:\n",
    "        # when there are multiple letters, there may be multiple sheets, or just one sheet. \n",
    "        # if there are multiple sheets, need to make sure the sheet matches the letter. There are only 3 of these\n",
    "        # should match these by hand.\n",
    "        if len(vote_data[d]['sheet_names'])>1:\n",
    "                print('has multiple sheets:',d)\n",
    "                \n",
    "        elif len(vote_data[d]['sheet_names'])<=1:\n",
    "            # when there are multiple letter but one sheet, easy to match each letter to the one sheet\n",
    "            #print('has one sheet, multiple letters:',d)\n",
    "            for l in letters:\n",
    "                vote_data[d]['props'][l] = {'s_name':vote_data[d]['sheet_names'][0]}\n",
    "\n",
    "    else:\n",
    "        # there's only one sheet, which will match the one letter. \n",
    "        vote_data[d]['props'][letters[0]]={'s_name':vote_data[d]['sheet_names'][0]}\n",
    "\n",
    "# where multiple sheets and mult letters, match by hand.\n",
    "vote_data['201511']['props']['D']={'s_name':'180 - Local Measure D'}\n",
    "vote_data['201511']['props']['I']={'s_name':'205 - Local Measure I'}\n",
    "vote_data['199911']['props']['H']={'s_name':'E to H','col_name':'PROP H'}\n",
    "vote_data['199911']['props']['I']={'s_name':'I to K','col_name':'PROP I'}\n",
    "vote_data['199911']['props']['J']={'s_name':'I to K','col_name':'PROP J'}\n",
    "vote_data['201311']['props']['B']={'s_name':'Measure A & B'}\n",
    "vote_data['201311']['props']['C']={'s_name':'Measure C & D'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the custom parameters we'll need to read in the excel files. \n",
    "vote_data = dpf.define_excel_params(vote_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read excel files for each election... \n",
    "for d in list(vote_data.keys()):\n",
    "    for l in vote_data[d]['props'].keys():\n",
    "        #print('\\n',d,l)\n",
    "        data=dpf.read_vote_sheet(d,l,vote_df=vote_data, path='../data/SOV_w_nimby/')\n",
    "        # test if already has multiindex:\n",
    "        if isinstance(data.index, pd.core.index.MultiIndex): \n",
    "            data=dpf.rename_index_and_cols(data)\n",
    "\n",
    "        else:\n",
    "            desc = dpf.check_if_descriptive(d)\n",
    "            data=dpf.format_df_to_multiindex(data, descriptive_labels=desc)\n",
    "            data=dpf.rename_index_and_cols(data)\n",
    "        \n",
    "        vote_data[d]['props'][l]['data']=data\n",
    "#data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify totals\n",
    "\n",
    "dpf.verify_vote_totals(vote_data, fname='verify_percentages.csv')\n",
    "\n",
    "# Check sums verify. Some of these are off. \n",
    "# For the 90s elections, there are three mysterious \"ballot types\" at the beginning that don't have a precinct.\n",
    "# that's what's throwing off the totals. Might be votes for people who don't vote at an address, like prisoners or something.\n",
    "# So just make sure that, when reporting full results, rely on the original sheet and not the dataframe. \n",
    "# otherwise everything checks out. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          voted  YES   NO  registered\n",
      "precinct                             \n",
      "PCT 1101    315  201   90         806\n",
      "PCT 1102    397  247  115         937\n",
      "PCT 1103    283  169   90         750\n",
      "PCT 1104    335  198  112         844\n",
      "PCT 1105    282  161   92         884\n",
      "PCT 1106    356  168  111         865\n",
      "PCT 1107    324  184  120         876\n",
      "PCT 1108    248  147   82         754\n",
      "PCT 1109    304  180  103         817\n",
      "PCT 1111    224  125   74         575\n",
      "               voted  YES   NO  registered\n",
      "precinct                                  \n",
      "PCT 2001         218   85  100         441\n",
      "PCT 2002         402  161  181        1006\n",
      "PCT 2003         355  133  175         778\n",
      "PCT 2004         275  103  128         645\n",
      "PCT 2005         415  173  197         972\n",
      "PCT 2006         255   80  136         563\n",
      "PCT 2007         296  104  156         672\n",
      "PCT 2008         266   90  137         599\n",
      "PCT 2009         419  164  195         932\n",
      "PCT 2011/2012    456  171  209        1090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vote_data1 = dpf.process_votedata(vote_data, dpf.consolidate_abs)\n",
    "vote_data1.keys()\n",
    "print(vote_data['200806']['props']['G']['data'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vote_data2 = dpf.process_votedata(vote_data1, dpf.make_vote_variables, use_datekey=True, use_propkey=True,df_prop=proposals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge voting data with census data by precinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8cc8c3d5d082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# format precinct names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvote_data3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_votedata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_data2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_precincts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# let's merge!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvote_data4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_votedata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_data3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_vote_census\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_datekey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisarayle/Dropbox/side_projects/sf_voting/src/data_prep_functions.py\u001b[0m in \u001b[0;36mprocess_votedata\u001b[0;34m(data, process_func, use_datekey, use_propkey, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m                     \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;31m#new dictionary copy with new dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mdata_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'props'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisarayle/Dropbox/side_projects/sf_voting/src/data_prep_functions.py\u001b[0m in \u001b[0;36mformat_precincts\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_prec_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0;31m#print(df_new.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisarayle/Dropbox/side_projects/sf_voting/src/data_prep_functions.py\u001b[0m in \u001b[0;36msplit_prec_rows\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;31m# delete original row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    337\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4219\u001b[0;31m                 \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4221\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, index, columns, **kwargs)\u001b[0m\n\u001b[1;32m   2682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m         return super(DataFrame, self).reindex(index=index, columns=columns,\n\u001b[0;32m-> 2684\u001b[0;31m                                               **kwargs)\n\u001b[0m\u001b[1;32m   2685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reindex_axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m         return self._reindex_axes(axes, level, limit, tolerance,\n\u001b[0;32m-> 1962\u001b[0;31m                                   method, fill_value, copy).__finalize__(self)\n\u001b[0m\u001b[1;32m   1963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m     def _reindex_axes(self, axes, level, limit, tolerance, method,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   2626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m             frame = frame._reindex_columns(columns, copy, level, fill_value,\n\u001b[0;32m-> 2628\u001b[0;31m                                            limit, tolerance)\n\u001b[0m\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_columns\u001b[0;34m(self, new_columns, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   2648\u001b[0m         new_columns, indexer = self.columns.reindex(new_columns, level=level,\n\u001b[1;32m   2649\u001b[0m                                                     \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2650\u001b[0;31m                                                     tolerance=tolerance)\n\u001b[0m\u001b[1;32m   2651\u001b[0m         return self._reindex_with_indexers({1: [new_columns, indexer]},\n\u001b[1;32m   2652\u001b[0m                                            \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/index.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/index.py\u001b[0m in \u001b[0;36m_ensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   6079\u001b[0m             \u001b[0mindex_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6080\u001b[0m         \u001b[0;31m# 2200 ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6081\u001b[0;31m         \u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_index_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6083\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_arrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# format precinct names \n",
    "vote_data3 = dpf.process_votedata(vote_data2, dpf.format_precincts)\n",
    "\n",
    "# let's merge!\n",
    "vote_data4 = dpf.process_votedata(vote_data3, dpf.merge_vote_census, use_datekey=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = vote_data4['200403']['props']['J']['data']\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for R, then save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataset that combines all elections, but make sure to have proposal and year dummies\n",
    "\n",
    "Create dummy variables:  \n",
    "- year\n",
    "\n",
    "- pres election year\n",
    "\n",
    "- november election\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vote_data5 = dpf.process_votedata(vote_data4, dpf.make_election_dummies, use_datekey=True, use_propkey=True)\n",
    "vote_data6 = dpf.process_votedata(vote_data5, dpf.fix_yr_built_moved, use_datekey=True)\n",
    "vote_data7 = dpf.process_votedata(vote_data6, dpf.adjust_inflation, use_datekey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = dpf.combine_dataframes(vote_data7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill na with False, for year dummies\n",
    "yr_cols = ['yr_1996', 'yr_1997', 'yr_1998','yr_1999', 'yr_2000', 'yr_2001', 'yr_2002', 'yr_2004', 'yr_2006',\n",
    "       'yr_2008', 'yr_2013', 'yr_2014', 'yr_2015']\n",
    "\n",
    "all_data[yr_cols] = all_data[yr_cols].fillna(value=False)\n",
    "\n",
    "# rename columns to get rid of \"_wgt\"\n",
    "all_data = dpf.rename_columns(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save!\n",
    "\n",
    "date.today().strftime('%m%d%Y')\n",
    "filepath = '../results/'\n",
    "d = date.today().strftime('%m%d%Y')\n",
    "filename = 'voting_data_all_{}.csv'.format(d)\n",
    "\n",
    "all_data.to_csv(filepath+filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    "yr_prop_cols = sorted(all_data.yr_prop.unique())\n",
    "for yp in yr_prop_cols: \n",
    "    print(yp, len(all_data[all_data.yr_prop==yp]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for visualization\n",
    "\n",
    "- Join result data with precincts, on prec ID, matching appropriate years.\n",
    "\n",
    "- Write to geojson files - 1 geojson file for each precinct layer (3 files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a column with year_month\n",
    "all_data['yr_mo'] = all_data['yr_prop'].str[:-1].astype(int)\n",
    "\n",
    "prec_keys=['pre1992','pre2002','pre2012']\n",
    "\n",
    "for prec_key in prec_keys:\n",
    "    yr = prec_key[-4:]\n",
    "    \n",
    "    # load precinct shapefiles as geodataframes\n",
    "    prec_df = load_prec_shp(yr)\n",
    "    \n",
    "    # filter all data based on the year\n",
    "    vote_df = dpf.filter_for_dates(all_data, prec_key)  \n",
    "    \n",
    "    # merge on precinct id \n",
    "    merged = pd.merge(prec_df, vote_df, on='precname')\n",
    "    \n",
    "    # remove unneeded columns to reduce file size\n",
    "    cols_to_drop = ['foreign_born','yr_1996', 'yr_1997', 'yr_1998','yr_1999', 'yr_2000', 'yr_2001', 'yr_2002', 'yr_2004', 'yr_2006',\n",
    "       'yr_2008', 'yr_2013', 'yr_2014', 'yr_2015']\n",
    "    try: \n",
    "        merged = merged.drop(cols_to_drop, axis=1)\n",
    "    except ValueError as err:\n",
    "        print('Error: {}'.format(err))\n",
    "    \n",
    "    # write to geojson by writing to a json string\n",
    "    path= '../results/maps/'\n",
    "    filename = 'results_{}.geojson'.format(prec_key)\n",
    "    \n",
    "    with open(path+filename, 'w') as f:\n",
    "        f.write(merged.to_json())\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
